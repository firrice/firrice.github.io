<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2022-Complex Intelligent Systems  Survey on clothing image retrieval with cross-domain</title>
    <link href="/posts/2024-06-30-1/"/>
    <url>/posts/2024-06-30-1/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>A survey aiming at achieving accurate clothing-retrieval in cross-domain situations.</p><h3 id="Survey-Results"><a href="#Survey-Results" class="headerlink" title="Survey Results"></a>Survey Results</h3><h4 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h4><ul><li><p>clothes are flexible items, and when viewed from different shooting angles or The appearance can be very different when wearing different body types.  </p></li><li><p>The intra-class variance is large and the inter-class variance is<br>small, which is an inherent characteristic of clothing images.</p></li></ul><h4 id="Previous-work"><a href="#Previous-work" class="headerlink" title="Previous work"></a>Previous work</h4><ol><li>critical region recognition</li></ol><ul><li>Bounding box </li><li>Humanbody landmark</li><li>Clothing landmark</li><li>Attention map</li></ul><p><img src="/post_imgs/20240630_1/1.png" alt=""></p><ol><li>Deep metric learning</li></ol><ul><li>Siamese network</li><li>Triplet network and variants</li><li>Ensemble network</li></ul><h4 id="Clothing-databases"><a href="#Clothing-databases" class="headerlink" title="Clothing databases"></a>Clothing databases</h4><ul><li>Street2Shop</li><li>DARN</li><li>DeepFashion v1&amp;v2 (Consumer-shop)</li><li>ModaNet</li></ul><p><img src="/post_imgs/20240630_1/2.png" alt="">  </p><p><img src="/post_imgs/20240630_1/3.png" alt=""></p><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ul><li>clothing landmark recognition and attention map recognition have a higher accuracy in clothing retrieval.   </li><li>deep metric learning is better in the same domain clothing image retrieval than the crossdomain clothing image retrieval.   </li><li>It can be seen that the overall effect of deep metric learning is not as good as clothing critical region recognition of solving cross-domain retrieval problems, indicating that the main problem to be solved in cross-domain clothing<br>retrieval are the recognition of clothing important regions in the image.   </li></ul><p><img src="/post_imgs/20240630_1/4.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Cross-domain clothing retrieval</category>
      
    </categories>
    
    
    <tags>
      
      <tag>image retrieval</tag>
      
      <tag>cross-domain</tag>
      
      <tag>clothing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023-ECML/PKDD Improving Position Encoding of Transformers for Multivariate Time Series Classification</title>
    <link href="/posts/2024-06-26-10/"/>
    <url>/posts/2024-06-26-10/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>The detailed study for position encoding within transformer to address MTSC problem is absent. So the paper tends to fill it in terms of Absolute Position Embedding(APE) and Relative Position Embedding(RPE). Next, authors analyse the shortcomings of them which were summarized as follows:  </p><ol><li><strong>APE</strong></li></ol><ul><li><p><strong>distance awareness property</strong>: It points to that the dot product doesn’t always decrease as the distance between two positions increases, which disappears when lower embedding dimensions, such as 64.</p></li><li><p><strong>anisotropic phenomenon</strong>: a significant part within the embedding vectors are similar between different positions, which will do harm to classification.</p></li></ul><p><img src="/post_imgs/20240626-10/1.png" alt=""></p><p><strong>note</strong>: distance awareness property and anisotropic phenomenon can be seen above.  </p><ol><li><strong>RPE</strong></li></ol><ul><li>lack of memory effiency: it requires $O({L^2}d)$ memory due to the additional relative position embedding.  </li></ul><p><img src="/post_imgs/20240626-10/2.png" alt=""></p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>The paper mainly tend to solve the problems above, and <strong>tAPE</strong>(Time Absolute Position Embedding) and <strong>eRPE</strong>(Efficient Relative Position Embedding) are proposed.  </p><ul><li>tAPE: It takes into account the input embedding dimension and length as follows:  </li></ul><p><img src="/post_imgs/20240626-10/3.png" alt=""></p><p>results:  </p><p><img src="/post_imgs/20240626-10/4.png" alt=""></p><p>According to the fig(a) above, the distance awareness property has been alleviated, and similarity has decreased in the fig(b) which means the aniostropic problem has been addressed to a certain degree.  </p><ul><li>eRPE</li></ul><p><img src="/post_imgs/20240626-10/5.png" alt=""></p><p><img src="/post_imgs/20240626-10/6.png" alt=""></p><p>results:  </p><p><img src="/post_imgs/20240626-10/7.png" alt=""></p><ul><li>The pipeline of <strong>ConvTran</strong> proposed:  </li></ul><p><img src="/post_imgs/20240626-10/8.png" alt=""></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/20240626-10/9.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MTSC</tag>
      
      <tag>position-info</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023-arxiv Enhancing Multivariate Time Series Classifiers through Self-Attention and Relative Positioning Infusion</title>
    <link href="/posts/2024-06-26-9/"/>
    <url>/posts/2024-06-26-9/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Two novel attention mechanisms were proposed, which contain a relative positive encoding and attention block.   </p><p>The main contributions:<br>(1)<strong>TPS</strong>: explicitly model the relative position distance into Gaussian distance!<br>(2)<strong>GTA</strong>: just like CA(coordinate attention) block in CV domain!   </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ul><li>Temporal Pseudo-Gaussian augmented Self-attention  </li></ul><p><img src="/post_imgs/20240626-9/1.png" alt=""></p><p><img src="/post_imgs/20240626-9/2.png" alt=""></p><p><img src="/post_imgs/20240626-9/3.png" alt=""></p><ul><li>Global Temporal Attention block  </li></ul><p><img src="/post_imgs/20240626-9/4.png" alt=""></p><p>For reference, the structure of CA(coordinate attention) block was as follow:  </p><p><img src="/post_imgs/20240626-9/5.png" alt=""> </p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/20240626-9/6.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MTSC</tag>
      
      <tag>position-info</tag>
      
      <tag>self-attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023-arxiv Bake off redux-a review and experimental evaluation of recent time series classification algorithms</title>
    <link href="/posts/2024-06-26-8/"/>
    <url>/posts/2024-06-26-8/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li>We extend the taxonomy to include three categories to reflect the recent development, which contain convolution、feature and deep learning.</li><li>We introduce 30 new datasets based on UCR112.</li><li>We make a summary of recent time series classification algorithms.</li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ol><li>The summary about recent TSC methods, which can be categorized to eight types based on <strong>representation of data</strong>:  </li></ol><ul><li>Distance: classification is based on some time series specific distance<br>measure between series</li><li>Feature: global features are extracted and passed to a standard classifier in a simple pipeline</li><li>Interval: features are derived from selected phase dependent intervals<br>in an ensemble of pipelines, and further output the summary statics.</li><li>Shapelet: phase independent discriminatory subseries form the basis<br>for classification(classify throught the existence of shapelet)</li><li>Dictionary: histograms of counts of repeating patterns are the features for a classifier</li><li>Convolution: convolutions and pooling operations create the feature<br>space for classification</li><li>Deep Learning: neural network</li><li>Hybrid: approaches combine two or more of the above approaches</li></ul><ol><li>The summary about recent TSC methods, which can be categorized to three types based on <strong>design patterns</strong>:  </li></ol><ul><li>single pipeline:transformations+standard ML classifier;</li><li>ensembles of pipelines;</li><li>transformations embedded in classifier structure, such decision tree,where the data is transformed at each node.</li></ul><p><strong>note</strong>: there are generally two kinds of transformations:<br>(1)series-to-vector,such as calculating summary features;<br>(2)series-to-series,such as transforming to the frequency domain of the series;   </p><ol><li>The eight types contain methods:  </li></ol><ul><li>Distance: 1-NN DTW, Elastic Ensemble(EE), Proximityforest(PF), ShapeDTW</li><li>Feature: The canonical time series characteristics(Catch22), Time Series Feature extraction based on Scalable Hypothesis Tests(TSFresh), Generalized signatures, FreshPRINCE</li><li>Interval: Time Series Forest(TSF), Random interval spectral ensemble(RISE), Supervised Time Series Forest(STSF), Random STSF(RSTSF), The Canonical Interval Forest(CIF), The diverse representation canonical interval forest(DrCIF)</li><li>Shapelet: The Shapelet Transform Classifer(STC), The Generalised Random Shapelet Forest(RSF), The Multible representation sequence learner(MrSEQL), MrSQM, Random Dilated shaplet transform(RDST)</li><li>Dictionary: Bag-of-SFA-Symbols(BOSS), Word Extraction for Time Series Classification(WEASEL), WEASEL with dilation(WEASEL-D), Contractble BOSS(cBOSS), Spatial BOSS, The Temporal Dictionary Ensemble(TDE)</li><li>Convolution: ROCKET, MiniRocket, MultiRocket, Hydra;</li><li>Deep learning: InceptionT, Resnet</li><li>Hybrid: HCa, HC1, HC2, TS-CHIEF</li></ul><p>The detailed information can be seen as follow:  </p><p><img src="/post_imgs/20240626-8/1.png" alt=""></p><h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><ul><li>30 new univariate datasets introduced:  </li></ul><p><img src="/post_imgs/20240626-8/2.png" alt=""></p><ul><li>Evaluation on UCR112:  </li></ul><p><img src="/post_imgs/20240626-8/3.png" alt=""></p><ul><li>Evaluation on UCR142:  </li></ul><p><img src="/post_imgs/20240626-8/4.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TSC</tag>
      
      <tag>survey</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022-ICLR OMNI-SCALE CNNS A SIMPLE AND EFFECTIVE KERNEL SIZE CONFIGURATION FOR TIME SERIES CLASSIFICATION</title>
    <link href="/posts/2024-06-26-7/"/>
    <url>/posts/2024-06-26-7/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p><img src="/post_imgs/20240626-7/1.png" alt=""></p><p>According to the left of fig(1), tuning RF size make substantial influence over acc, and there is no single best RF size across all datasets demonstarted in the right of fig(1). So researchers seeked to design a method to find each best RF size for every datasets. And next, they have found two phenomena as can been below, which help propose the 1D-OS block in the paper.  </p><p><img src="/post_imgs/20240626-7/2.png" alt=""></p><ul><li>Phenomenon 1st: according to the left of Fig2，<strong>1D-CNNs are not sensitive to the specific kernel size configurations</strong>.</li><li>Phenomenon 2nd: according to the right of Fig2，<strong>the performance of 1D-CNNs is mainly determined by the best RF size it has</strong>.</li><li>note: we can also find that the model’s performance is positive correlation with the model’s RF size.</li></ul><p>According to the finds above, what we need to do is to design a method which can cover all RF sizes, then it will has similar performance with the best RF size, and there is no additional consider for specific kernel configuration. So, the OS block was proposed based on <strong>Goldbach’s conjecture</strong>, where any positive even number can be written as the sum of two prime numbers under this background.   </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><img src="/post_imgs/20240626-7/3.png" alt=""></p><p><img src="/post_imgs/20240626-7/4.png" alt=""></p><p><img src="/post_imgs/20240626-7/5.png" alt=""></p><p>As we can seen in the figure above, the set <strong>S</strong> cover all interger RF sizes, and <strong>this design manner has advantages in terms of model size and scalability for long time series due to the property of prime number</strong> which can be seen below:   </p><p><img src="/post_imgs/20240626-7/6.png" alt=""></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/20240626-7/7.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TSC</tag>
      
      <tag>cnn-kernel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021-arxiv Gated Transformer Networks for Multivariate Time Series Classification</title>
    <link href="/posts/2024-06-26-6/"/>
    <url>/posts/2024-06-26-6/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>In this work, researchers explored a simple extension of the current Transformer Networks with gating, named Gated Transformer Networks (GTN) for the multivariate time series classification problem.</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>Actually, a two-towers transformers acquiring position &amp; channel attention utilizing transposing time and position axis was proposed, and the pipeline was as follow:   </p><p><img src="/post_imgs/20240626-6/1.png" alt=""></p><p><img src="/post_imgs/20240626-6/2.png" alt=""></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/20240626-6/3.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MTSC</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021-KDD A TRANSFORMER-BASED FRAMEWORK FOR MULTIVARIATE TIME SERIES REPRESENTATION LEARNING</title>
    <link href="/posts/2024-06-26-5/"/>
    <url>/posts/2024-06-26-5/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>In this paper, a transformer-based framework for unsupervised learning of multivariate time series was proposed for the first time.</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>The overall pipeline is as follow:   </p><p><img src="/post_imgs/20240626-5/1.png" alt=""></p><p>As we can see in the figure above, the pipeline consists of two phases:   </p><p>（1）<strong>Pre-train</strong>   </p><ul><li>masked input</li><li>Pre-train a encoder-only transformer-based model to make the autoregressive objective on the traininig set.</li><li>MSE loss was calculated pointed at masked position</li></ul><p>（2）<strong>Fine-tune</strong>  </p><ul><li>Add a Linear layer to project the representation learned into the output space for regression/classification tasks.  </li></ul><p><strong>Comparison with original encoder-only model</strong>:   </p><ul><li>normalization for each dimension across all training samples</li><li>input was firstly linearly projected to a d-dimensional vector space, which is a bit like embedding layer in LLM</li><li>using position embedding instead of position encoding</li><li>padding mask: shorter samples are padded with arbitrary values, and we generate a padding mask which adds a large negative value to the attention scores</li><li>using batch normalization rather than layer normalization</li><li>Full-parameter finetune is better than freezon finetune</li></ul><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/20240626-5/2.png" alt=""> </p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MTSC</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020-IJCAI A New Attention Mechanism to Classify Multivariate Time Series</title>
    <link href="/posts/2024-06-26-4/"/>
    <url>/posts/2024-06-26-4/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>First, the long-range dependencies of the time-series sequences are not well captured. Second, the interactions of multiple variables are generally not represented in features. To address these aforementioned issues, the paper propose a novel Cross Attention Stabilized Fully Convolutional Neural Network (CA-SFCN) to classify MTS data.</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>The main contributions are summarized as follow:   </p><p>A “novel” attention mechanism was proposed which actually is just self-attention beyond to transformers, while applied to X and Y axis over feature maps. Consequently, attention across time axis and channel axis was modeled, so the method in this paper was called cross-attention.   </p><p>The overall pipeline can be seen below:   </p><p><img src="/post_imgs/20240626-4/1.png" alt=""></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/20240626-4/2.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MTSC</tag>
      
      <tag>attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2019-NIPs Unsupervised Scalable Representation Learning for Multivariate Time Series</title>
    <link href="/posts/2024-06-26-3/"/>
    <url>/posts/2024-06-26-3/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>There exists <strong>highly variable lengths</strong> and <strong>sparse labeling</strong> in practice, to tackle the issues, a kind of unsupervised scalable representation was proposed this paper.</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ul><li>Backbone module uses unsupervised learning on training set(Encoder-only architecture, which contains causal Conv), the comparison between causal conv and LSTM can be seen as below:   </li></ul><p><img src="/post_imgs/20240626-3/1.png" alt=""></p><ul><li>Projection head(like: SVM) uses supervised learning on training set;</li><li>A novel triplet loss, which samples the negative and positive pair based on the subseries from the respective sample(positive/negative sample), the relative pseudo-code is as follow:   </li></ul><p><img src="/post_imgs/20240626-3/2.png" alt=""></p><p><img src="/post_imgs/20240626-3/3.png" alt=""></p><p>The formula of triplet loss introduced is as follow:  </p><p><img src="/post_imgs/20240626-3/4.png" alt=""></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/20240626-3/5.png" alt=""></p><p><img src="/post_imgs/20240626-3/6.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TSC</tag>
      
      <tag>unsupervised learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2018-arxiv The UEA multivariate time series classification archive(2018)</title>
    <link href="/posts/2024-06-26-UEA/"/>
    <url>/posts/2024-06-26-UEA/</url>
    
    <content type="html"><![CDATA[<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>It’s the first iteration of MTSC datasets, which contains 30 datasets with equal series length per problem. The MTSC datasets of UEA archive consist of three types of sources, they are HAR(Human activity Recognition), Motion Classification, and ECG Classification. The detailed information can be seen as follows:   </p><p><img src="/post_imgs/20240626-2/1.png" alt=""></p><p>Further, authors make evaluation based on 1-NN, DTW. The results are as below:  </p><p><img src="/post_imgs/20240626-2/2.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TSC</tag>
      
      <tag>survey</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2017-ICDM Generating synthetic time series to augment sparse datasets</title>
    <link href="/posts/2024-06-26-1/"/>
    <url>/posts/2024-06-26-1/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>A new synthetic series genaration method was proposed, and the average time series based on the extention of DBA(Dynamic Barycenter Averaging) was as a synthetic sample.  </p><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><p>The paper firstly anlysed two shortcomings using the original DBA to generate synthetic samples, which contained as below:   </p><ul><li>Modifying the starting time series(that is the initial random average series in DBA) is not sufficient to create enough <strong>diversity</strong> in the synthesized dataset;</li><li>The raw DBA method which utilizes the “uniformly weighted” schedule may generate <strong>undesirable</strong> samples when the distribution is not spherical. Such as for “U-shape” distribution, computing the center of this U will typically construct very unlikely objects.</li></ul><p>The main contributes of this paper were to adress two problems above through designing a <strong>weighted</strong> schedule(for the second problem) based on <strong>batch</strong> series rather than all samples in DBA to induce the diversity(for the first problem). The weighted mechanism was introdued as follow:   </p><p><img src="/post_imgs/20240626-1/1.png" alt=""></p><p>Fuether, three extentions of DBA concentrating on “Batch” were proposed, and which were introduced as follow detailedly:   </p><ol><li>Average All (AA)</li></ol><p><img src="/post_imgs/20240626-1/2.png" alt=""></p><ol><li>Average Selected (AS)</li></ol><p><img src="/post_imgs/20240626-1/3.png" alt=""></p><ol><li>Average Selected with Distance (ASD)</li></ol><p>It adds the density property to distribution based on distance to Nearst Neighbor.   </p><p><img src="/post_imgs/20240626-1/4.png" alt=""> </p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>1、<strong>ASD is better than AA</strong>  </p><p><img src="/post_imgs/20240626-1/5.png" alt=""> </p><p>2、<strong>Increasing the number of available examples per class can strongthen model</strong>.   </p><p><img src="/post_imgs/20240626-1/6.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Time Series Classification</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series Generating</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023-arxiv Retrieval-Augmented Generation for Large Language Models-A Survey</title>
    <link href="/posts/2024-06-26-RAG-survey/"/>
    <url>/posts/2024-06-26-RAG-survey/</url>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>The paper mainly introcude the solution for optimizing the RAG system, and the evaluation framework&amp;tools are also included, which can be seen as below:    </p><p><img src="/post_imgs/RAG-survey/1.png" alt=""></p><p>More specifically, three paradigms of RAG are demonstrated as followings:   </p><p><img src="/post_imgs/RAG-survey/2.png" alt=""></p><p><img src="/post_imgs/RAG-survey/3.png" alt=""></p><p>The ecosystem of RAG can be seen in the img below:   </p><p><img src="/post_imgs/RAG-survey/4.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Large Language Model</category>
      
    </categories>
    
    
    <tags>
      
      <tag>survey</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024-NIPS QLoRA-Efficient Finetuning of Quantized LLMs</title>
    <link href="/posts/2024-06-26-QLoRA/"/>
    <url>/posts/2024-06-26-QLoRA/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>A new efficient finetuning method based on quantile quantization was proposed, which realized the goal finetuning a 65B model on a single 48GB GPU, preserving full 16-bit task performance at the same time.</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>Core concepts:   </p><ul><li>4-bit NormalFloat Quantization(NF4)   </li><li>Double Quantization   </li><li>Paged optimizer   </li></ul><h4 id="4-bit-NormalFloat-Quantization-NF4"><a href="#4-bit-NormalFloat-Quantization-NF4" class="headerlink" title="4-bit NormalFloat Quantization(NF4)"></a><strong>4-bit NormalFloat Quantization(NF4)</strong></h4><p><strong>The final goal</strong>   </p><p>FP32 -&gt; INT4, which the latter has a range of &#91;-8, 7&#93;(the first bit is sign bit, and “1000” &amp; “0000” represent the value of zero, it was agreed that “1000” points to “-8”, and “0000” points to “0”)  </p><p><strong>The overrall pipeline</strong></p><p>(1) estimate the ${2^{\rm{k}}} + 1$ quantiles of a theoretical N(0, 1) distribution to obtain a k-bit quantile quantization data type for normal distributions;   </p><p>(2) take this data type and normalize its values into the [−1, 1] range;   </p><p>(3) quantize an input weight tensor by normalizing it into the [−1, 1] range through absolute maximum rescaling.<br>(4) quantization inference.   </p><p>The detailed explaination can refer to: <a href="https://readpaper.feishu.cn/docx/CrMGdSVPKow5d1x1XQMcJioRnQe">link1</a>,<a href="https://zhuanlan.zhihu.com/p/638927564">link2</a>   </p><p>Here, we have:   </p><p><img src="/post_imgs/QLoRA/1.jpg" alt=""></p><p>The process of getting 16 quantiles can refer to the code below:   </p><p><img src="/post_imgs/QLoRA/2.png" alt=""></p><p>The process of quantization inference:   </p><p><img src="/post_imgs/QLoRA/3.png" alt=""></p><h4 id="Double-Quantization"><a href="#Double-Quantization" class="headerlink" title="Double Quantization"></a><strong>Double Quantization</strong></h4><p>The core of it is to quantize the <strong>constant</strong>!   </p><p><img src="/post_imgs/QLoRA/4.jpg" alt=""></p><h4 id="Paged-optimizer"><a href="#Paged-optimizer" class="headerlink" title="Paged optimizer"></a><strong>Paged optimizer</strong></h4><p>Excerpted from the raw article: “We use this feature to allocate paged memory for the optimizer states which are then <strong>automatically evicted to CPU RAM</strong> when the GPU runs out-of-memory and paged back into GPU memory when the memory is needed in the optimizer update step.”   </p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>Which can refer to the raw paper.</p>]]></content>
    
    
    <categories>
      
      <category>Large Language Model</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021-ICLR LORA LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</title>
    <link href="/posts/2024-06-26-LoRA/"/>
    <url>/posts/2024-06-26-LoRA/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Finetuning full-parameter model is expensive.</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>Using the low-rank A&amp;B to represent the finetuned parameters, the detailed pipeline is as follow:  </p><p><img src="/post_imgs/LoRA/1.png" alt=""></p><p><img src="/post_imgs/LoRA/2.png" alt=""></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>There are really abundant experiments for LoRA, and the detail can reference to the original paper.</p>]]></content>
    
    
    <categories>
      
      <category>Large Language Model</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022-ACL/short P-Tuning v2 Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</title>
    <link href="/posts/2024-06-26-P-Tuning-v2/"/>
    <url>/posts/2024-06-26-P-Tuning-v2/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>In the context of NLU tasks, the prior work reveals that prompt-tuning <strong>doesn’t perform well for normal-sized pretrained model and hard sequence labeling tasks</strong>(that means difficult tasks, like: NER, etc). To address the problems, we perform a optimization version of prompt-tuning to strengthen the <strong>scalability</strong> and adaption for <strong>multi-tasks</strong>.   </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>Technically, P-tuning v2 is not conceptually novel, it can be viewed as an optimized and adapted implementation of prefix-tuning. The pipeline was depicted as blow:   </p><p><img src="/post_imgs/P-tuning-v2/1.png" alt=""></p><p>Note, there are several differences between P-tuning v2 and Prefix-tuning, which was summarized as follows:    </p><ol><li><p><strong>Applied tasks</strong>: Prefix-tuning focuses on NLG tasks using only-decoder or encode-decoder, while P-tuning v2 extend to NLU and NLG tasks.  </p></li><li><p><strong>Reparameterization</strong>: we discover that its usefulness depends on tasks and datasets in the paper. And it seems like that the reparameterization(MLP head in Prefix-tuning) was removed in pratice.   </p></li><li><p><strong>Classification Head</strong>: Generally, using a language modeling head to predict verbalizers. While P-tuning v2 used random initialized classification head on top of the tokens.   </p></li></ol><p>Below demonstrates the comparison between P-tuning v2 with existing Prompt-tuning approches.   </p><p><img src="/post_imgs/P-tuning-v2/2.png" alt=""></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/P-tuning-v2/3.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Large Language Model</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021-arxiv P-tuning v1 GPT Understands, Too</title>
    <link href="/posts/2024-06-25-P-tuning-v1/"/>
    <url>/posts/2024-06-25-P-tuning-v1/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Manual discrete prompts have several drawbacks, which contain unstable performance(eg: changing a single word causes substantial drow) . To adress the problem, we propose a novel method which employs trainable continuous prompt embeddings in concatenation with discrete prompts.  </p><p>Unstable performance of discrete prompts can be seen as follow:  </p><ol><li>change a single word make large influence to prediction.<br><img src="/post_imgs/P-tuning-v1/1.png" alt=""></li></ol><p><strong>note</strong>: when the LLM is tuned, the instability can be alleviated, but performance difference is still sizeable, especially in few-shot. And recent algorithms do not well with it.  </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ol><li><p>Firstly, the work flow of discrete prompts is as follow:  </p></li><li><p>Step1: Let M be a pretrained language model with a hidden size of h and a vocabulary size of |V|. Let {(xi, yi))}i be a labeled dataset for an NLU task, where x0:n = {x0, x1, …, xn} is an input consisting of a sequence of discrete tokens, and y ∈ Y is a label. Our goal is to estimate the conditional probability for classification fM(x) = pˆ(y|x) with parameters of M either finetuned or frozen.</p></li><li>Step2: Let [Di] be a discrete prompt token. Each prompt can be described as a template T = {[D0:i], x, [D(i+1):j], y, [D(j+1):k]}</li><li>Step3: Through the work above, the labeled data can be organized into a sequence of text tokens including x and y, and the task can be reformulated as filling in the blanks of the input text.   </li></ol><p>Eg: For the task of predicting a country’s capital (LAMA-TREx P36), a prompt could be “The capital of [INPUT] is [LABEL].” With a piece of labeled data “(Britain, London)”, the reformulated text would be “The capital of Britain is [MASK].”, where “[MASK]” should predict the given label “London”.   </p><ol><li>The pipeline proposed in the paper and comparison with discrete prompts is as follow:  </li></ol><p><img src="/post_imgs/P-tuning-v1/2.png" alt=""></p><p>Let [Pi] be the ith continuous prompt embedding. The prompt template for P-tuning is as follows:  </p><p>T = {[P0:i], x, [P(i+1):j], y, [P(j+1):k]}   </p><p>P-Tuning leverages an extra embedding function f : [Pi] → hi to map the template to:   </p><p>{h0, …, hi, e(x), hi+1, …, hj, e(y), hj+1, …, hk}   </p><p>Finally, we update the embeddings $P_i^k$ to optimize a task loss function. </p><p>Note: For the choose of [Pi], they can be extracted from <strong>unused tokens</strong> belong to pretraining vocabulary, eg: unused 1 ~ unused99 in BERT vob.   </p><ol><li>Prompt Encoder Network</li></ol><p>There are three types of encoder for continuous embedding, they are LSTM, MLP and EMB. EMB means identity map and the ablation is as follow:   </p><p><img src="/post_imgs/P-tuning-v1/3.png" alt=""></p><p>Based on the comparison above, <strong>LSTM</strong> and <strong>MLP</strong> generally works well on these tasks, and EMB can be substantially under-perform the other two on some tasks.   </p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/P-tuning-v1/4.png" alt=""></p><p>More evaluation results can be found in the paper.</p>]]></content>
    
    
    <categories>
      
      <category>Large Language Model</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Blogs in AI domain</title>
    <link href="/posts/2024-06-25-Blogs/"/>
    <url>/posts/2024-06-25-Blogs/</url>
    
    <content type="html"><![CDATA[<h3 id="Large-Language-Model"><a href="#Large-Language-Model" class="headerlink" title="Large Language Model"></a>Large Language Model</h3><ul><li><a href="https://lilianweng.github.io/posts/">Lilian Weng</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Blog links</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog links</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021-ACL/IJCNLP Prefix-Tuning Optimizing Continuous Prompts for Generation</title>
    <link href="/posts/2024-06-24-Prefix-Tuning/"/>
    <url>/posts/2024-06-24-Prefix-Tuning/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Fine-tuning has to train and store a full copy of the model parameter, which leads to high compution complexity and large storing resource. To address the problem, Prefix-tuning was proposed to optimize a small continuous task-specific vector(called prefix), while keeping the LLM parameters frozen.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>Generally, to make the light-weight finetune, researchers make large efforts on “inserting” the task-specific parameters into LM. There are several types of “inserting” which contains:(1)adapter-tuning:insert layers between LLM’layers;(2)Prompt engineer: design the task-specific prompt. So, it seems like that, PEFT strategy is to learning “Bias”(in downstream tasks). Several general fine-tuning methods are modeled as follows. Such as table-to-text task, $x$ corresponds to a linearized data table and $y$ is a textual description.  </p><ol><li>Autoregressive LM<br><img src="/post_imgs/Prefix-Tuning/1.png" alt=""><br>As shown in the figure above, let $z = [x;y]$ be the concatenation of $x$ and $y$; let ${X<em>{idx}}$ denote the sequence of indices that corresponds to $x$, and ${Y</em>{idx}}$ denote the same for $y$. The activation at time step $i$ is ${h_i} \in {R^d}$, where ${h_i} = \left[ {h_i^{(1)};{\rm{ }}\cdot{\rm{ }}\cdot{\rm{ }}\cdot{\rm{ }};h_i^{(n)}} \right]$ is a concatenation of all activation layers at this time step, and $h_i^{(j)}$ is the activation of the $j$-th Transformer layer at time step $i$.  </li></ol><p><img src="/post_imgs/Prefix-Tuning/2.png" alt=""></p><ol><li>Encoder-Decoder</li></ol><p><img src="/post_imgs/Prefix-Tuning/3.png" alt=""></p><ol><li>Fine-tuning</li></ol><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><p>The pipeline of the Prefix-tuning can be seen as follows:<br><img src="/post_imgs/Prefix-Tuning/4.png" alt=""><br><img src="/post_imgs/Prefix-Tuning/5.png" alt=""></p><p><strong>Note</strong>:<br>(1) prefix-tuning don’t optimize the ${P_\theta }$ due to performance dropping, it makes through <strong>MLP</strong>.<br><img src="/post_imgs/Prefix-Tuning/6.png" alt=""><br><img src="/post_imgs/Prefix-Tuning/7.png" alt=""></p><p>(2) It inserts prefix to every layer of Decoder, rather than only in embedding layer.<br>(3) Random initialization leads to a low performance with high variance! So, Initialize the prefix with activations of real words significantly improves generation.   </p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="Table-to-Text-task"><a href="#Table-to-Text-task" class="headerlink" title="Table-to-Text task"></a>Table-to-Text task</h4><p><img src="/post_imgs/Prefix-Tuning/8.png" alt=""> </p><h4 id="Summarization-task"><a href="#Summarization-task" class="headerlink" title="Summarization task"></a>Summarization task</h4><p><img src="/post_imgs/Prefix-Tuning/9.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Large Language Model</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024-06-23-hello-world/"/>
    <url>/2024-06-23-hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
