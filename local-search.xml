<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2023-arxiv Retrieval-Augmented Generation for Large Language Models-A Survey</title>
    <link href="/2024/06/26/RAG-survey/"/>
    <url>/2024/06/26/RAG-survey/</url>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>The paper mainly introcude the solution for optimizing the RAG system, and the evaluation framework&amp;tools are also included, which can be seen as below:    </p><p><img src="/post_imgs/RAG-survey/1.png" alt=""></p><p>More specifically, three paradigms of RAG are demonstrated as followings:   </p><p><img src="/post_imgs/RAG-survey/2.png" alt=""></p><p><img src="/post_imgs/RAG-survey/3.png" alt=""></p><p>The ecosystem of RAG can be seen in the img below:   </p><p><img src="/post_imgs/RAG-survey/4.png" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>RAG</tag>
      
      <tag>survey</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024-NIPS QLoRA-Efficient Finetuning of Quantized LLMs</title>
    <link href="/2024/06/26/QLoRA/"/>
    <url>/2024/06/26/QLoRA/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>A new efficient finetuning method based on quantile quantization was proposed, which realized the goal finetuning a 65B model on a single 48GB GPU, preserving full 16-bit task performance at the same time.</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>Core concepts:   </p><ul><li>4-bit NormalFloat Quantization(NF4)   </li><li>Double Quantization   </li><li>Paged optimizer   </li></ul><h4 id="4-bit-NormalFloat-Quantization-NF4"><a href="#4-bit-NormalFloat-Quantization-NF4" class="headerlink" title="4-bit NormalFloat Quantization(NF4)"></a><strong>4-bit NormalFloat Quantization(NF4)</strong></h4><p><strong>The final goal</strong>   </p><p>FP32 -&gt; INT4, which the latter has a range of &#91;-8, 7&#93;(the first bit is sign bit, and “1000” &amp; “0000” represent the value of zero, it was agreed that “1000” points to “-8”, and “0000” points to “0”)  </p><p><strong>The overrall pipeline</strong></p><p>(1) estimate the ${2^{\rm{k}}} + 1$ quantiles of a theoretical N(0, 1) distribution to obtain a k-bit quantile quantization data type for normal distributions;   </p><p>(2) take this data type and normalize its values into the [−1, 1] range;   </p><p>(3) quantize an input weight tensor by normalizing it into the [−1, 1] range through absolute maximum rescaling.<br>(4) quantization inference.   </p><p>The detailed explaination can refer to: <a href="https://readpaper.feishu.cn/docx/CrMGdSVPKow5d1x1XQMcJioRnQe">link1</a>,<a href="https://zhuanlan.zhihu.com/p/638927564">link2</a>   </p><p>Here, we have:   </p><p><img src="/post_imgs/QLoRA/1.jpg" alt=""></p><p>The process of getting 16 quantiles can refer to the code below:   </p><p><img src="/post_imgs/QLoRA/2.png" alt=""></p><p>The process of quantization inference:   </p><p><img src="/post_imgs/QLoRA/3.png" alt=""></p><h4 id="Double-Quantization"><a href="#Double-Quantization" class="headerlink" title="Double Quantization"></a><strong>Double Quantization</strong></h4><p>The core of it is to quantize the <strong>constant</strong>!   </p><p><img src="/post_imgs/QLoRA/4.jpg" alt=""></p><h4 id="Paged-optimizer"><a href="#Paged-optimizer" class="headerlink" title="Paged optimizer"></a><strong>Paged optimizer</strong></h4><p>Excerpted from the raw article: “We use this feature to allocate paged memory for the optimizer states which are then <strong>automatically evicted to CPU RAM</strong> when the GPU runs out-of-memory and paged back into GPU memory when the memory is needed in the optimizer update step.”   </p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>Which can refer to the raw paper.</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021-ICLR LORA LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</title>
    <link href="/2024/06/26/LoRA/"/>
    <url>/2024/06/26/LoRA/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Finetuning full-parameter model is expensive.</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>Using the low-rank A&amp;B to represent the finetuned parameters, the detailed pipeline is as follow:  </p><p><img src="/post_imgs/LoRA/1.png" alt=""></p><p><img src="/post_imgs/LoRA/2.png" alt=""></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>There are really abundant experiments for LoRA, and the detail can reference to the original paper.</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022-ACL/short P-Tuning v2 Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</title>
    <link href="/2024/06/26/P-tuning-v2/"/>
    <url>/2024/06/26/P-tuning-v2/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>In the context of NLU tasks, the prior work reveals that prompt-tuning <strong>doesn’t perform well for normal-sized pretrained model and hard sequence labeling tasks</strong>(that means difficult tasks, like: NER, etc). To address the problems, we perform a optimization version of prompt-tuning to strengthen the <strong>scalability</strong> and adaption for <strong>multi-tasks</strong>.   </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>Technically, P-tuning v2 is not conceptually novel, it can be viewed as an optimized and adapted implementation of prefix-tuning. The pipeline was depicted as blow:   </p><p><img src="/post_imgs/P-tuning-v2/1.png" alt=""></p><p>Note, there are several differences between P-tuning v2 and Prefix-tuning, which was summarized as follows:    </p><ol><li><p><strong>Applied tasks</strong>: Prefix-tuning focuses on NLG tasks using only-decoder or encode-decoder, while P-tuning v2 extend to NLU and NLG tasks.  </p></li><li><p><strong>Reparameterization</strong>: we discover that its usefulness depends on tasks and datasets in the paper. And it seems like that the reparameterization(MLP head in Prefix-tuning) was removed in pratice.   </p></li><li><p><strong>Classification Head</strong>: Generally, using a language modeling head to predict verbalizers. While P-tuning v2 used random initialized classification head on top of the tokens.   </p></li></ol><p>Below demonstrates the comparison between P-tuning v2 with existing Prompt-tuning approches.   </p><p><img src="/post_imgs/P-tuning-v2/2.png" alt=""></p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/P-tuning-v2/3.png" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021-arxiv P-tuning v1 GPT Understands, Too</title>
    <link href="/2024/06/25/P-tuning-v1/"/>
    <url>/2024/06/25/P-tuning-v1/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Manual discrete prompts have several drawbacks, which contain unstable performance(eg: changing a single word causes substantial drow) . To adress the problem, we propose a novel method which employs trainable continuous prompt embeddings in concatenation with discrete prompts.  </p><p>Unstable performance of discrete prompts can be seen as follow:  </p><ol><li>change a single word make large influence to prediction.<br><img src="/post_imgs/P-tuning-v1/1.png" alt=""></li></ol><p><strong>note</strong>: when the LLM is tuned, the instability can be alleviated, but performance difference is still sizeable, especially in few-shot. And recent algorithms do not well with it.  </p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ol><li><p>Firstly, the work flow of discrete prompts is as follow:  </p></li><li><p>Step1: Let M be a pretrained language model with a hidden size of h and a vocabulary size of |V|. Let {(xi, yi))}i be a labeled dataset for an NLU task, where x0:n = {x0, x1, …, xn} is an input consisting of a sequence of discrete tokens, and y ∈ Y is a label. Our goal is to estimate the conditional probability for classification fM(x) = pˆ(y|x) with parameters of M either finetuned or frozen.</p></li><li>Step2: Let [Di] be a discrete prompt token. Each prompt can be described as a template T = {[D0:i], x, [D(i+1):j], y, [D(j+1):k]}</li><li>Step3: Through the work above, the labeled data can be organized into a sequence of text tokens including x and y, and the task can be reformulated as filling in the blanks of the input text.   </li></ol><p>Eg: For the task of predicting a country’s capital (LAMA-TREx P36), a prompt could be “The capital of [INPUT] is [LABEL].” With a piece of labeled data “(Britain, London)”, the reformulated text would be “The capital of Britain is [MASK].”, where “[MASK]” should predict the given label “London”.   </p><ol><li>The pipeline proposed in the paper and comparison with discrete prompts is as follow:  </li></ol><p><img src="/post_imgs/P-tuning-v1/2.png" alt=""></p><p>Let [Pi] be the ith continuous prompt embedding. The prompt template for P-tuning is as follows:  </p><p>T = {[P0:i], x, [P(i+1):j], y, [P(j+1):k]}   </p><p>P-Tuning leverages an extra embedding function f : [Pi] → hi to map the template to:   </p><p>{h0, …, hi, e(x), hi+1, …, hj, e(y), hj+1, …, hk}   </p><p>Finally, we update the embeddings $P_i^k$ to optimize a task loss function. </p><p>Note: For the choose of [Pi], they can be extracted from <strong>unused tokens</strong> belong to pretraining vocabulary, eg: unused 1 ~ unused99 in BERT vob.   </p><ol><li>Prompt Encoder Network</li></ol><p>There are three types of encoder for continuous embedding, they are LSTM, MLP and EMB. EMB means identity map and the ablation is as follow:   </p><p><img src="/post_imgs/P-tuning-v1/3.png" alt=""></p><p>Based on the comparison above, <strong>LSTM</strong> and <strong>MLP</strong> generally works well on these tasks, and EMB can be substantially under-perform the other two on some tasks.   </p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p><img src="/post_imgs/P-tuning-v1/4.png" alt=""></p><p>More evaluation results can be found in the paper.</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021-ACL/IJCNLP Prefix-Tuning Optimizing Continuous Prompts for Generation</title>
    <link href="/2024/06/24/Prefix-Tuning/"/>
    <url>/2024/06/24/Prefix-Tuning/</url>
    
    <content type="html"><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>Fine-tuning has to train and store a full copy of the model parameter, which leads to high compution complexity and large storing resource. To address the problem, Prefix-tuning was proposed to optimize a small continuous task-specific vector(called prefix), while keeping the LLM parameters frozen.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>Generally, to make the light-weight finetune, researchers make large efforts on “inserting” the task-specific parameters into LM. There are several types of “inserting” which contains:(1)adapter-tuning:insert layers between LLM’layers;(2)Prompt engineer: design the task-specific prompt. So, it seems like that, PEFT strategy is to learning “Bias”(in downstream tasks). Several general fine-tuning methods are modeled as follows. Such as table-to-text task, $x$ corresponds to a linearized data table and $y$ is a textual description.  </p><ol><li>Autoregressive LM<br><img src="/post_imgs/Prefix-Tuning/1.png"><br>As shown in the figure above, let $z &#x3D; [x;y]$ be the concatenation of $x$ and $y$; let ${X_{idx}}$ denote the sequence of indices that corresponds to $x$, and ${Y_{idx}}$ denote the same for $y$. The activation at time step $i$ is ${h_i} \in {R^d}$, where ${h_i} &#x3D; \left[ {h_i^{(1)};{\rm{ }}\cdot{\rm{ }}\cdot{\rm{ }}\cdot{\rm{ }};h_i^{(n)}} \right]$ is a concatenation of all activation layers at this time step, and $h_i^{(j)}$ is the activation of the $j$-th Transformer layer at time step $i$.</li></ol><p><img src="/post_imgs/Prefix-Tuning/2.png"></p><ol start="2"><li>Encoder-Decoder</li></ol><p><img src="/post_imgs/Prefix-Tuning/3.png"></p><ol start="3"><li>Fine-tuning</li></ol><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><p>The pipeline of the Prefix-tuning can be seen as follows:<br><img src="/post_imgs/Prefix-Tuning/4.png"><br><img src="/post_imgs/Prefix-Tuning/5.png"></p><p><strong>Note</strong>:<br>(1) prefix-tuning don’t optimize the ${P_\theta }$ due to performance dropping, it makes through <strong>MLP</strong>.<br><img src="/post_imgs/Prefix-Tuning/6.png"><br><img src="/post_imgs/Prefix-Tuning/7.png"></p><p>(2) It inserts prefix to every layer of Decoder, rather than only in embedding layer.<br>(3) Random initialization leads to a low performance with high variance! So, Initialize the prefix with activations of real words significantly improves generation.   </p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="Table-to-Text-task"><a href="#Table-to-Text-task" class="headerlink" title="Table-to-Text task"></a>Table-to-Text task</h4><p><img src="/post_imgs/Prefix-Tuning/8.png"> </p><h4 id="Summarization-task"><a href="#Summarization-task" class="headerlink" title="Summarization task"></a>Summarization task</h4><p><img src="/post_imgs/Prefix-Tuning/9.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM-finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/06/23/hello-world/"/>
    <url>/2024/06/23/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
